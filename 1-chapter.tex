\newpage
\section{\Large Алгоритмы классификации данных}
Классификация данных состоит из прогнозирования определенного результата на основе уже известных данных. Чтобы предсказать результат, алгоритм обрабатывает данные, содержащие набор атрибутов и соответствующий каждому набору результат, обычно называемый атрибутом прогнозирования цели или классом~\cite{hetal2016}. Формируется модель алгоритма, которая пытается обнаружить отношения между атрибутами, которые позволили бы предсказать результат~\cite{kumbhar}. Такая процедура называется обучением модели, а набор данных, используемый для этого -- тестовой выборкой~\cite{Mirmozaffari}.
\par
Следующим шагом после обучения модели является прогнозирование -- процедура определения класса, при которой используется набор данных с неизвестными классами. Такой набор данных, который содержит тот же набор атрибутов, за исключением атрибута прогнозирования, часто называют тестовой выборкой~\cite{tprogeralgorithms}.
\par
Алгоритм анализирует входные данные и выдает прогноз. Точность прогноза определяет, насколько «хорош» алгоритм. Например, в медицинской базе данных обучающий набор должен иметь соответствующую информацию о пациенте, записанную ранее, где атрибутом прогноза является наличие или отсутствие у пациента проблем со здоровьем.
\par
Для определения того, какой именно алгоритм использовать для конкретной задачи можно воспользоваться схемой, изображенной на рисунке~\ref{ris:scikitlearn-map}. Исходя из того, что в текущей задаче используется не тестовая информация и имеется 160 примеров, то были выбраны алгоритмы SVM, k-ближайших соседей и наивный байесовский классификатор, описанные ниже.
\\
\imgh{1\linewidth}{scikitlearn-map}{Схема для определения алгоритма классификации для конкретной задачи}
\subsection{Метод опорных векторов}
Метод опорных вектором или SVM -- это метод статистической классификации~\cite{kristianini}. Он широко используется для задач различного рода и хорошо себя в них показывает~\cite{crammer}.
\par
Основной идеей метода является представление атрибутов данных в виде векторов и переход в пространство более высокой размерности, чем получившееся на этапе представления векторами. Затем ищется гиперплоскость с максимальным зазором в пространстве между объектами разных классов~\cite{rashka}~\cite{statnikov}.
\par
На рисунке~\ref{ris:svm_example} показан пример классификации методом SVM. Красной линией выделена как раз та самая гиперплоскость, четко разделяющая объекты разных классов друг от друга.
\\
\imgh{1\linewidth}{svm_example}{Пример классификации методом SVM}
\par
Алгоритм может использоваться с одним из следующих видов ядер~\cite{crammer}:
\begin{itemize}
	\item[-] Полиномиальное (однородное) $k(x,x^{'}) = (x \cdot x^{'})^{d}$;
	\item[-] Полиномиальное (неоднородное) $k(x,x^{'}) = (x \cdot x^{'} + 1)^{d}$;
	\item[-] Радиальная базисная функция $k(x,x^{'}) = exp(-\gamma ||x - x^{'}||^{2})$, для $\gamma > 0$;
	\item[-] Радиальная базисная функция Гаусса $k(x,x^{'}) = exp\left ( -\frac{||x - x^{'}||^{2}}{2\sigma^{2}} \right )$;
	\item[-] Сигмоид $k(x,x^{'}) = tanh(kx\cdot x^{'} + c)$.
\end{itemize}
\subsection{Метод k-ближайших соседей}
Алгоритм k-ближайших соседей является простым статистическим алгоритмом обучения, в котором объект классифицируется своими соседями. При классификации таким методом объект относится к классу, наиболее распространенному среди его k-ближайших соседей~\cite{potapov}~\cite{flah}. Пример классификации приведен на рисунке~\ref{ris:knn_example}, где в качестве классифицируемого объекта используется прямоугольник и существует несколько объектов известных классов -- белые точки и черные. Замерив расстояние от объекта до его соседей с различными классами и основываясь на методе k-ближайших соседей данный объект будет отнесен к классу черный точек, а не белых.
\\
\imgh{1\linewidth}{knn_example}{Пример классификации методом k-ближайших соседей}
\par
При нахождении атрибутов учитывается значимость атрибутов и часто применяется прием растяжения осей, демонстрируемый в формуле~\eqref{eq:eq1}. Использование данного приема снижает ошибку классификации.
\begin{equation}\label{eq:eq1}
D_{E} = \sqrt{3(x_{A} - y_{A})^{2} + (x_{B} - y_{B})^{2}},
\end{equation}
где $x_{A}, y_{A}$ -- значения атрибута A в наборе данных, $x_{B}, y_{B}$ -- значения атрибута B.
\par
Данный алгоритм возможно применять как для данных с маленьким количеством атрибутов, так и с достаточно большим. Важным моментом при работе с алгоритом является определение функции расстояния между значениями. Примером такой функции может быть евклидово расстояние -- формула~\eqref{eq:eq2}.
\begin{equation}\label{eq:eq2}
D_{E} = \sqrt{\sum_{i}^{n}(x_{i} - y_{i})^{2}},
\end{equation}
где $x_{i}, y_{i}$ -- значения атрибутов в наборе данных.
\subsection{Наивный байесовский классификатор}
Наивный байесовский классификатор является простым вероятностным классификатором и основывается на применении теоремы Байеса со строгими предположениями о независимости~\cite{potapov}~\cite{danilovsv}. Хотя наивный байесовский классификатор редко применим к большинству реальных задач, но зачастую в определенных задачах он демонстрирует хорошие результаты и часто конкурирует с более сложными методами, такими как SVM и классификационным деревьями~\cite{Mirmozaffari}. Классификация данным методом очень зависит от распределения зависимостей атрибутов, а не от самих зависимостей~\cite{juravlev}.
\par
Вероятностная модель классификатора:
\begin{equation}\label{eq:eq3}
p(C|F_{1},...,F_{n}) = \frac{p(C)p(F_{1},...,F_{n}|C)}{p(F_{1},...,F_{n})},
\end{equation}
где $C$ -- класс модели, а $F_{i}$ -- классифицируемые модели~\cite{Mirmozaffari}.
\par
Использование формулы~\eqref{eq:eq3} при классификации дает минимально значение среднего риска или математичского ожидания ошибки:
\begin{equation}\label{eq:eq4}
R(a) = \sum_{y\in Y}\sum_{\varsigma\in Y}\lambda_{y}P_{y}P_{x,y}\left\lbrace a(x)=\varsigma|y \right\rbrace ,
\end{equation}
где $\lambda_{y}$ -- цена ошибки при отнесении объекта класса $Y$ к какому-либо другому классу.
%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------
